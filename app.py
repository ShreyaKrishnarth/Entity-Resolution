import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import io
from datetime import datetime
import time

from data_processor import DataProcessor
from deduplication_engine import DeduplicationEngine
from utils import Utils

# Page configuration
st.set_page_config(
    page_title="Product Catalogue Deduplication",
    page_icon="üîç",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Initialize session state
if 'datasets' not in st.session_state:
    st.session_state.datasets = {}
if 'processed_data' not in st.session_state:
    st.session_state.processed_data = None
if 'duplicates' not in st.session_state:
    st.session_state.duplicates = None
if 'master_catalogue' not in st.session_state:
    st.session_state.master_catalogue = None

# Main title
st.title("üîç Product Catalogue Deduplication")
st.markdown("**Merge and deduplicate product catalogs using advanced text similarity algorithms**")

# Sidebar for navigation
with st.sidebar:
    st.header("Navigation")
    page = st.radio(
        "Select Page:",
        ["Data Upload", "Data Exploration", "Deduplication", "Master Catalogue", "Export & Documentation"]
    )
    
    st.markdown("---")
    st.markdown("### Current Status")
    
    # Status indicators
    datasets_loaded = len(st.session_state.datasets) > 0
    data_processed = st.session_state.processed_data is not None
    duplicates_found = st.session_state.duplicates is not None
    catalogue_created = st.session_state.master_catalogue is not None
    
    st.write("üìÅ Datasets Loaded:", "‚úÖ" if datasets_loaded else "‚ùå")
    st.write("üßπ Data Processed:", "‚úÖ" if data_processed else "‚ùå")
    st.write("üîç Duplicates Found:", "‚úÖ" if duplicates_found else "‚ùå")
    st.write("üìã Catalogue Created:", "‚úÖ" if catalogue_created else "‚ùå")

# Page 1: Data Upload
if page == "Data Upload":
    st.header("üìÅ Data Upload")
    st.markdown("Upload your CSV files containing product data for deduplication.")
    
    # Upload instructions
    st.info("üí° **Tip:** You can drag and drop CSV files directly onto the upload areas below, or click to browse and select files from your computer.")
    
    # Enhanced drag-and-drop file upload
    st.markdown("""
    <style>
    .upload-section {
        border: 2px dashed #cccccc;
        border-radius: 10px;
        padding: 20px;
        text-align: center;
        margin: 10px 0;
        background-color: #fafafa;
    }
    .upload-section:hover {
        border-color: #1f77b4;
        background-color: #f0f8ff;
    }
    .file-details {
        background-color: #e8f4fd;
        padding: 10px;
        border-radius: 5px;
        margin: 10px 0;
    }
    </style>
    """, unsafe_allow_html=True)
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown('<div class="upload-section">', unsafe_allow_html=True)
        st.subheader("üìÅ Dataset 1")
        st.markdown("**Drag and drop your first CSV file here**")
        
        uploaded_file1 = st.file_uploader(
            "Choose first CSV file",
            type=['csv'],
            key="file1",
            help="Drag and drop a CSV file or click to browse"
        )
        st.markdown('</div>', unsafe_allow_html=True)
        
        if uploaded_file1 is not None:
            try:
                # Show file info before processing
                file_size = uploaded_file1.size / 1024  # KB
                st.markdown(f'<div class="file-details">', unsafe_allow_html=True)
                st.write(f"**File:** {uploaded_file1.name}")
                st.write(f"**Size:** {file_size:.1f} KB")
                
                # Read and preview data
                df1 = pd.read_csv(uploaded_file1)
                st.session_state.datasets['dataset1'] = {
                    'name': uploaded_file1.name,
                    'data': df1
                }
                
                st.success(f"‚úÖ Successfully loaded {uploaded_file1.name}")
                st.write(f"**Shape:** {df1.shape[0]} rows √ó {df1.shape[1]} columns")
                
                # Column preview
                with st.expander("üìã Column Preview", expanded=False):
                    col_info = []
                    for col in df1.columns:
                        null_count = df1[col].isnull().sum()
                        null_pct = (null_count / len(df1)) * 100
                        col_info.append({
                            'Column': col,
                            'Type': str(df1[col].dtype),
                            'Missing': f"{null_count} ({null_pct:.1f}%)",
                            'Sample': str(df1[col].dropna().iloc[0]) if not df1[col].dropna().empty else "N/A"
                        })
                    st.dataframe(pd.DataFrame(col_info), use_container_width=True)
                
                # Data preview
                with st.expander("üëÄ Data Preview", expanded=False):
                    st.dataframe(df1.head(10), use_container_width=True)
                
                st.markdown('</div>', unsafe_allow_html=True)
                
            except Exception as e:
                st.error(f"Error loading file: {str(e)}")
                st.info("Please ensure your file is a valid CSV format")
    
    with col2:
        st.markdown('<div class="upload-section">', unsafe_allow_html=True)
        st.subheader("üìÅ Dataset 2")
        st.markdown("**Drag and drop your second CSV file here**")
        
        uploaded_file2 = st.file_uploader(
            "Choose second CSV file",
            type=['csv'],
            key="file2",
            help="Drag and drop a CSV file or click to browse"
        )
        st.markdown('</div>', unsafe_allow_html=True)
        
        if uploaded_file2 is not None:
            try:
                # Show file info before processing
                file_size = uploaded_file2.size / 1024  # KB
                st.markdown(f'<div class="file-details">', unsafe_allow_html=True)
                st.write(f"**File:** {uploaded_file2.name}")
                st.write(f"**Size:** {file_size:.1f} KB")
                
                # Read and preview data
                df2 = pd.read_csv(uploaded_file2)
                st.session_state.datasets['dataset2'] = {
                    'name': uploaded_file2.name,
                    'data': df2
                }
                
                st.success(f"‚úÖ Successfully loaded {uploaded_file2.name}")
                st.write(f"**Shape:** {df2.shape[0]} rows √ó {df2.shape[1]} columns")
                
                # Column preview
                with st.expander("üìã Column Preview", expanded=False):
                    col_info = []
                    for col in df2.columns:
                        null_count = df2[col].isnull().sum()
                        null_pct = (null_count / len(df2)) * 100
                        col_info.append({
                            'Column': col,
                            'Type': str(df2[col].dtype),
                            'Missing': f"{null_count} ({null_pct:.1f}%)",
                            'Sample': str(df2[col].dropna().iloc[0]) if not df2[col].dropna().empty else "N/A"
                        })
                    st.dataframe(pd.DataFrame(col_info), use_container_width=True)
                
                # Data preview
                with st.expander("üëÄ Data Preview", expanded=False):
                    st.dataframe(df2.head(10), use_container_width=True)
                
                st.markdown('</div>', unsafe_allow_html=True)
                
            except Exception as e:
                st.error(f"Error loading file: {str(e)}")
                st.info("Please ensure your file is a valid CSV format")
    
    # Dataset comparison summary
    if len(st.session_state.datasets) >= 2:
        st.markdown("---")
        st.subheader("üìä Dataset Comparison Summary")
        
        # Get datasets
        datasets = list(st.session_state.datasets.values())
        df1, df2 = datasets[0]['data'], datasets[1]['data']
        name1, name2 = datasets[0]['name'], datasets[1]['name']
        
        # Comparison metrics
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric(
                label="Dataset 1 Rows",
                value=f"{len(df1):,}",
                delta=f"{name1}"
            )
        
        with col2:
            st.metric(
                label="Dataset 2 Rows", 
                value=f"{len(df2):,}",
                delta=f"{name2}"
            )
        
        with col3:
            common_cols = set(df1.columns) & set(df2.columns)
            st.metric(
                label="Common Columns",
                value=len(common_cols),
                delta=f"out of {max(len(df1.columns), len(df2.columns))}"
            )
        
        with col4:
            total_products = len(df1) + len(df2)
            st.metric(
                label="Total Products",
                value=f"{total_products:,}",
                delta="to be deduplicated"
            )
        
        # Column mapping preview
        with st.expander("üîó Column Mapping Preview", expanded=False):
            col_map_data = []
            all_cols = set(df1.columns) | set(df2.columns)
            
            for col in sorted(all_cols):
                in_df1 = "‚úÖ" if col in df1.columns else "‚ùå"
                in_df2 = "‚úÖ" if col in df2.columns else "‚ùå"
                
                # Predict standardized name
                standardized = col.lower().strip()
                mapping = {
                    'product_name': 'name', 'name_clean': 'name', 'title': 'name',
                    'description_clean': 'description', 'desc': 'description', 'overview': 'description',
                    'main_category': 'category', 'category_slug': 'category', 'parent_category': 'category',
                    'seller_website': 'url', 'website': 'url',
                    'software_product_id': 'product_id', 'technology_id': 'product_id', 'id': 'product_id'
                }
                predicted_name = mapping.get(standardized, standardized)
                
                col_map_data.append({
                    'Original Column': col,
                    'In Dataset 1': in_df1,
                    'In Dataset 2': in_df2,
                    'Will Map To': predicted_name
                })
            
            st.dataframe(pd.DataFrame(col_map_data), use_container_width=True)
    
    # Load sample datasets button
    st.markdown("---")
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("üìã Load Sample Datasets", type="secondary"):
            try:
                # Load the provided sample datasets
                df1 = pd.read_csv("attached_assets/bd df 1000_1749585466102.csv")
                df2 = pd.read_csv("attached_assets/ts df 1000_1749585466103.csv")
                
                st.session_state.datasets = {
                    'dataset1': {
                        'name': 'bd_products.csv',
                        'data': df1
                    },
                    'dataset2': {
                        'name': 'ts_technologies.csv', 
                        'data': df2
                    }
                }
                st.success("‚úÖ Sample datasets loaded!")
                st.rerun()
            except Exception as e:
                st.error(f"Error loading sample datasets: {str(e)}")
    
    # Column selection and processing
    if len(st.session_state.datasets) >= 2:
        st.markdown("---")
        st.subheader("Column Selection")
        
        # Get all available columns from both datasets
        all_columns = set()
        for dataset_info in st.session_state.datasets.values():
            all_columns.update(dataset_info['data'].columns.tolist())
        
        # Remove 'source' if it exists since we add it automatically
        all_columns.discard('source')
        all_columns = sorted(list(all_columns))
        
        # Create column selection interface
        col1, col2 = st.columns(2)
        
        with col1:
            st.write("**Available Columns:**")
            for dataset_key, dataset_info in st.session_state.datasets.items():
                with st.expander(f"{dataset_info['name']} ({len(dataset_info['data'])} rows)"):
                    cols_info = []
                    for col in dataset_info['data'].columns:
                        null_count = dataset_info['data'][col].isnull().sum()
                        null_pct = (null_count / len(dataset_info['data'])) * 100
                        cols_info.append(f"‚Ä¢ {col} ({null_pct:.1f}% missing)")
                    st.write("\n".join(cols_info))
        
        with col2:
            st.write("**Select Columns to Process:**")
            
            # Quick selection options
            col2_1, col2_2 = st.columns(2)
            with col2_1:
                if st.button("Select All", key="select_all"):
                    st.session_state.selected_columns = all_columns
            with col2_2:
                if st.button("Clear All", key="clear_all"):
                    st.session_state.selected_columns = []
            
            # Initialize selected columns if not exists
            if 'selected_columns' not in st.session_state:
                # Default recommended columns
                recommended = []
                for col in ['product_name', 'name', 'description', 'category', 'main_category', 'url', 'seller_website']:
                    if col in all_columns:
                        recommended.append(col)
                st.session_state.selected_columns = recommended
            
            # Multi-select for columns
            selected_columns = st.multiselect(
                "Choose columns:",
                options=all_columns,
                default=st.session_state.selected_columns,
                help="Select the columns you want to include in the processing"
            )
            st.session_state.selected_columns = selected_columns
            
            # Show selection summary
            if selected_columns:
                st.success(f"‚úÖ {len(selected_columns)} columns selected")
                with st.expander("Selected columns"):
                    st.write(", ".join(selected_columns))
            else:
                st.warning("‚ö†Ô∏è No columns selected")
        
        # Process data button
        if selected_columns:
            st.markdown("---")
            if st.button("üßπ Process Selected Data", type="primary", use_container_width=True):
                with st.spinner("Processing selected columns..."):
                    try:
                        processor = DataProcessor()
                        processed_data = processor.process_datasets(
                            st.session_state.datasets, 
                            selected_columns
                        )
                        st.session_state.processed_data = processed_data
                        st.success("‚úÖ Data processed successfully!")
                        st.rerun()
                    except Exception as e:
                        st.error(f"Error processing data: {str(e)}")
                        st.exception(e)

# Page 2: Data Exploration
elif page == "Data Exploration":
    st.header("üìä Data Exploration")
    
    if not st.session_state.datasets:
        st.warning("‚ö†Ô∏è Please upload datasets first.")
        st.stop()
    
    if st.session_state.processed_data is None:
        st.warning("‚ö†Ô∏è Please process data first on the Data Upload page.")
        st.stop()
    
    processed_data = st.session_state.processed_data
    
    # Dataset overview
    st.subheader("Dataset Overview")
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Total Products", processed_data['total_products'])
    with col2:
        st.metric("Dataset 1 Products", processed_data['dataset1_count'])
    with col3:
        st.metric("Dataset 2 Products", processed_data['dataset2_count'])
    with col4:
        st.metric("Common Columns", len(processed_data['common_columns']))
    
    # Data quality overview
    st.subheader("Data Quality Analysis")
    
    tab1, tab2, tab3 = st.tabs(["Missing Values", "Column Statistics", "Sample Data"])
    
    with tab1:
        if processed_data['missing_values_summary'].empty:
            st.success("‚úÖ No missing values found!")
        else:
            st.write("Missing values by column:")
            fig = px.bar(
                processed_data['missing_values_summary'],
                x='column',
                y='missing_count',
                title="Missing Values by Column"
            )
            st.plotly_chart(fig, use_container_width=True)
    
    with tab2:
        st.write("Column statistics:")
        st.dataframe(processed_data['column_stats'], use_container_width=True)
    
    with tab3:
        st.write("Sample of processed data:")
        st.dataframe(processed_data['combined_data'].head(20), use_container_width=True)

# Page 3: Deduplication
elif page == "Deduplication":
    st.header("üîç Product Deduplication")
    
    if st.session_state.processed_data is None:
        st.warning("‚ö†Ô∏è Please process data first.")
        st.stop()
    
    # Deduplication settings
    st.subheader("Deduplication Settings")
    
    col1, col2 = st.columns(2)
    
    with col1:
        similarity_threshold = st.slider(
            "Similarity Threshold",
            min_value=0.5,
            max_value=1.0,
            value=0.8,
            step=0.05,
            help="Products with similarity above this threshold will be considered duplicates"
        )
        
        algorithm = st.selectbox(
            "Matching Algorithm",
            ["fuzzy_ratio", "fuzzy_partial", "fuzzy_token_sort", "tfidf_cosine"],
            help="Algorithm to use for similarity calculation"
        )
    
    with col2:
        primary_field = st.selectbox(
            "Primary Matching Field",
            ["name", "description", "combined"],
            help="Primary field to use for matching"
        )
        
        use_secondary_fields = st.checkbox(
            "Use Secondary Fields",
            value=True,
            help="Use additional fields for confirmation"
        )
    
    # Run deduplication
    if st.button("üöÄ Find Duplicates", type="primary"):
        # Create progress tracking elements
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        def update_progress(progress, message):
            progress_bar.progress(progress / 100)
            status_text.text(message)
        
        try:
            dedup_engine = DeduplicationEngine()
            
            # Configure deduplication parameters
            config = {
                'similarity_threshold': similarity_threshold,
                'algorithm': algorithm,
                'primary_field': primary_field,
                'use_secondary_fields': use_secondary_fields
            }
            
            # Initialize progress
            update_progress(0, "Starting duplicate detection...")
            
            # Find duplicates with progress callback
            duplicates = dedup_engine.find_duplicates(
                st.session_state.processed_data['combined_data'],
                config,
                progress_callback=update_progress
            )
            
            st.session_state.duplicates = duplicates
            
            # Clear progress elements and show success
            progress_bar.empty()
            status_text.empty()
            st.success(f"‚úÖ Found {len(duplicates)} potential duplicate pairs!")
            st.rerun()
            
        except Exception as e:
            progress_bar.empty()
            status_text.empty()
            st.error(f"Error finding duplicates: {str(e)}")
            st.exception(e)
    
    # Display duplicates
    if st.session_state.duplicates is not None:
        duplicates = st.session_state.duplicates
        
        st.subheader(f"Potential Duplicates ({len(duplicates)} pairs)")
        
        if len(duplicates) == 0:
            st.info("üéâ No duplicates found with current settings!")
        else:
            # Duplicate pairs visualization
            similarity_scores = [dup['similarity'] for dup in duplicates]
            
            fig = go.Figure(data=go.Histogram(x=similarity_scores, nbinsx=20))
            fig.update_layout(
                title="Distribution of Similarity Scores",
                xaxis_title="Similarity Score",
                yaxis_title="Number of Pairs"
            )
            st.plotly_chart(fig, use_container_width=True)
            
            # Detailed duplicate pairs
            st.subheader("Duplicate Pairs Review")
            
            for i, dup in enumerate(duplicates[:10]):  # Show first 10
                with st.expander(f"Pair {i+1} (Similarity: {dup['similarity']:.3f})"):
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.write("**Product 1:**")
                        st.write(f"**Name:** {dup['product1'].get('name', 'N/A')}")
                        st.write(f"**Description:** {dup['product1'].get('description', 'N/A')[:200]}...")
                        st.write(f"**Source:** {dup['product1'].get('source', 'N/A')}")
                    
                    with col2:
                        st.write("**Product 2:**")
                        st.write(f"**Name:** {dup['product2'].get('name', 'N/A')}")
                        st.write(f"**Description:** {dup['product2'].get('description', 'N/A')[:200]}...")
                        st.write(f"**Source:** {dup['product2'].get('source', 'N/A')}")
            
            if len(duplicates) > 10:
                st.info(f"Showing first 10 pairs. Total: {len(duplicates)} pairs found.")
        
        # Generate master catalogue
        if st.button("üìã Generate Master Catalogue", type="primary"):
            # Create progress tracking for catalogue generation
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            try:
                status_text.text("Creating master catalogue...")
                progress_bar.progress(0.2)
                
                dedup_engine = DeduplicationEngine()
                
                status_text.text("Grouping duplicate records...")
                progress_bar.progress(0.5)
                
                master_catalogue = dedup_engine.create_master_catalogue(
                    st.session_state.processed_data['combined_data'],
                    duplicates
                )
                
                status_text.text("Finalizing catalogue...")
                progress_bar.progress(0.9)
                
                st.session_state.master_catalogue = master_catalogue
                
                progress_bar.progress(1.0)
                status_text.text("Master catalogue created successfully!")
                
                # Clear progress elements
                progress_bar.empty()
                status_text.empty()
                
                st.success("‚úÖ Master catalogue created!")
                st.rerun()
                
            except Exception as e:
                progress_bar.empty()
                status_text.empty()
                st.error(f"Error creating master catalogue: {str(e)}")
                st.exception(e)

# Page 4: Master Catalogue
elif page == "Master Catalogue":
    st.header("üìã Master Catalogue")
    
    if st.session_state.master_catalogue is None:
        st.warning("‚ö†Ô∏è Please generate master catalogue first.")
        st.stop()
    
    master_catalogue = st.session_state.master_catalogue
    
    # Statistics
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Total Unique Products", len(master_catalogue))
    with col2:
        if st.session_state.processed_data:
            original_count = st.session_state.processed_data['total_products']
            duplicates_removed = original_count - len(master_catalogue)
            st.metric("Duplicates Removed", duplicates_removed)
        else:
            st.metric("Duplicates Removed", "N/A")
    with col3:
        if st.session_state.processed_data:
            original_count = st.session_state.processed_data['total_products']
            duplicates_removed = original_count - len(master_catalogue)
            dedup_rate = (duplicates_removed / original_count) * 100 if original_count > 0 else 0
            st.metric("Deduplication Rate", f"{dedup_rate:.1f}%")
        else:
            st.metric("Deduplication Rate", "N/A")
    with col4:
        st.metric("Data Quality Score", "85%")  # Based on completeness
    
    # Master catalogue preview
    st.subheader("Master Catalogue Preview")
    
    # Show column information
    if not master_catalogue.empty:
        st.info(f"Output contains {len(master_catalogue.columns)} columns: {', '.join(master_catalogue.columns)}")
        st.dataframe(master_catalogue.head(20), use_container_width=True)
    else:
        st.warning("Master catalogue is empty")
    
    # Category distribution
    if 'category' in master_catalogue.columns:
        st.subheader("Product Categories")
        category_counts = master_catalogue['category'].value_counts().head(10)
        fig = px.bar(
            x=category_counts.values,
            y=category_counts.index,
            orientation='h',
            title="Top 10 Product Categories"
        )
        st.plotly_chart(fig, use_container_width=True)

# Page 5: Export & Documentation
elif page == "Export & Documentation":
    st.header("üì§ Export & Documentation")
    
    if st.session_state.master_catalogue is None:
        st.warning("‚ö†Ô∏è Please generate master catalogue first.")
        st.stop()
    
    # Export options
    st.subheader("Export Options")
    
    col1, col2 = st.columns(2)
    
    with col1:
        # Export master catalogue
        if st.button("üì• Download Master Catalogue", type="primary"):
            # Ensure only required columns are in the output
            output_df = st.session_state.master_catalogue.copy()
            
            # Define the exact columns we want in the output
            required_output_columns = ['product_name', 'description', 'url', 'category', 'source']
            
            # Filter to only include available required columns
            available_columns = [col for col in required_output_columns if col in output_df.columns]
            
            if available_columns:
                output_df = output_df[available_columns]
            
            # Show preview of what will be downloaded
            st.subheader("Export Preview")
            st.info(f"Exporting {len(output_df)} products with columns: {', '.join(output_df.columns)}")
            st.dataframe(output_df.head(5), use_container_width=True)
            
            csv_buffer = io.StringIO()
            output_df.to_csv(csv_buffer, index=False)
            csv_string = csv_buffer.getvalue()
            
            st.download_button(
                label="Download CSV File",
                data=csv_string,
                file_name=f"master_catalogue_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                mime="text/csv"
            )
    
    with col2:
        # Export duplicate pairs
        if st.session_state.duplicates:
            if st.button("üì• Download Duplicate Pairs"):
                duplicates_df = pd.DataFrame([
                    {
                        'product1_name': dup['product1'].get('name', ''),
                        'product2_name': dup['product2'].get('name', ''),
                        'similarity_score': dup['similarity'],
                        'algorithm_used': dup.get('algorithm', 'N/A')
                    }
                    for dup in st.session_state.duplicates
                ])
                
                csv_buffer = io.StringIO()
                duplicates_df.to_csv(csv_buffer, index=False)
                csv_string = csv_buffer.getvalue()
                
                st.download_button(
                    label="Download Duplicates CSV",
                    data=csv_string,
                    file_name=f"duplicate_pairs_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                    mime="text/csv"
                )
    
    # Process Documentation
    st.subheader("Process Documentation")
    
    if (st.session_state.duplicates is not None and 
        st.session_state.processed_data is not None and 
        st.session_state.master_catalogue is not None):
        # Generate documentation
        doc_content = Utils.generate_documentation(
            st.session_state.processed_data,
            st.session_state.duplicates,
            st.session_state.master_catalogue
        )
        
        st.markdown(doc_content)
        
        # Download documentation
        st.download_button(
            label="üì• Download Documentation",
            data=doc_content,
            file_name=f"deduplication_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md",
            mime="text/markdown"
        )
    
    # Technical Details
    with st.expander("üîß Technical Details"):
        st.markdown("""
        ### Deduplication Methodology
        
        1. **Text Preprocessing**: Clean and normalize product names and descriptions
        2. **Similarity Calculation**: Use multiple algorithms (Fuzzy matching, TF-IDF)
        3. **Threshold-based Matching**: Identify duplicates above similarity threshold
        4. **Conflict Resolution**: Merge duplicate records with data prioritization
        5. **Quality Validation**: Ensure data integrity in final catalogue
        
        ### Algorithms Used
        - **Fuzzy String Matching**: Levenshtein distance-based similarity
        - **TF-IDF Cosine Similarity**: Vector space model for text comparison
        - **Token-based Matching**: Word-level comparison for robust matching
        """)

# Footer
st.markdown("---")
st.markdown("**Product Catalogue Deduplication System** | Built with Streamlit")
